# YOLOv8 Car Detection

This project focuses on training a YOLOv8 model to detect cars, primarily for scenarios like parking lot monitoring. It includes scripts for data preparation, training, evaluation, and a simple API server for model deployment.

## Project Goal

To develop and deploy a YOLOv8 model capable of accurately detecting cars in images, particularly from high-angle perspectives simulating parking lot cameras. The project utilizes MLflow for experiment tracking and Grad-CAM for model interpretability.

## Directory Structure

- **/dataset/**: Contains the raw and annotated image data for training and testing.
- **/eda.py**: Script for performing Exploratory Data Analysis on the dataset (e.g., counting instances of cars).
- **/gran-cam-output/**: Stores the output images from Grad-CAM analysis, helping to visualize what the model focuses on.
- **/gran-cam.py**: Script to generate Grad-CAM visualizations for trained YOLOv8 models.
- **/mlartifacts/**: Stores artifacts generated by MLflow runs, such as trained models, configuration files, and other outputs. This directory is managed by MLflow.
- **/mlruns/**: Contains metadata, parameters, metrics, and tags for MLflow experiments. This directory is managed by MLflow.
- **/mlflow_setup.py**: A utility script that might be used to configure or start an MLflow tracking server locally.
- **/server/**: Contains the FastAPI application for serving the trained YOLOv8 model via a REST API.
- **/test_local.py**: Script to test a trained model (loaded from MLflow) on local images or videos.
- **/video_to_image.py**: Script to extract individual frames from video files and save them as images.
- **/yolo_cam/**: Module based on [YOLOv8_Explainer](https://github.com/Spritan/YOLOv8_Explainer) to integrate Grad-CAM with YOLOv8 models.
- **/yolo_training.py**: Main script for training the YOLOv8 model.
- **/requirements.txt**: Lists the Python dependencies required for this project.

## Setup and Usage Steps

1.  **Clone the Repository:**

    ```bash
    git clone https://github.com/ReggieReo/ku-parking-ai-component
    cd ku-parking-ai-component
    ```

2.  **Create a Virtual Environment and Install Dependencies:**

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    pip install -r requirements.txt
    ```

3.  **Obtain Dataset:**

    - Use a provided dataset by placing it in the `dataset/` directory.
    - Alternatively, collect your own images (e.g., high-angle photos simulating parking lot cameras) and place them in `dataset/images/`.

4.  **(Optional) Video to Image Conversion:**
    If your data is in video format, use the `video_to_image.py` script to extract frames:

    ```bash
    python video_to_image.py --video_dir sample_data --output_dir extracted_frames_output --frame_skip 60
    ```

    Ensure the output images are placed in a structured way within `dataset/` (e.g., `dataset/images/train`, `dataset/images/val`).

5.  **Annotate Images:**
    Use an annotation tool (e.g., LabelImg, Roboflow, CVAT) to create YOLO format labels for your images. Annotations should typically be stored in a corresponding `labels/` directory (e.g., `dataset/labels/train`, `dataset/labels/val`). Each image `image_name.jpg` should have a corresponding `image_name.txt` label file.

6.  **(Optional) Exploratory Data Analysis (EDA):**
    Run the `eda.py` script to analyze your dataset:

    ```bash
    python eda.py --dataset_path dataset/
    ```

7.  **Setup and Run MLflow:**
    Start the MLflow tracking server. You can use the local filesystem or a dedicated server.
    For local tracking, often no explicit server start is needed if `yolo_training.py` is configured to log locally.
    If you have `mlflow_setup.py` or use a separate server:

    ```bash
    mlflow ui
    ```

    This will start the MLflow UI, typically at `http://localhost:5000`.
    Ensure your training script (`yolo_training.py`) is configured to log to this MLflow instance.

8.  **Run Training:**
    Execute the `yolo_training.py` script. You might need to configure parameters within the script (e.g., dataset path, model configuration, epochs, batch size).

    ```bash
    python yolo_training.py
    ```

    (Adjust parameters as per your `yolo_training.py` script's requirements).
    Experiments will be logged to MLflow.

9.  **Test Model Locally:**
    Use the `test_local.py` script to try out a trained model. You'll typically need the model URI or path from an MLflow run.

    ```bash
    python test_local.py --model_path "runs:/<MLFLOW_RUN_ID>/model" --source path/to/test_image.jpg_or_video.mp4
    ```

    Or if using a local .pt file:

    ```bash
    python test_local.py --model_path path/to/your/best.pt --source path/to/test_image.jpg_or_video.mp4
    ```

    Refer to `test_local.py` for specific arguments.

10. **Generate Grad-CAM Visualizations (Optional):**
    Use `gran-cam.py` (or functionality within `yolo_cam/`) to generate Grad-CAM heatmaps.

    ```bash
    python gran-cam.py
    ```

    (Adjust command based on the script's actual arguments).

11. **Deploy with Server:**
    Navigate to the `server/` directory and run the application (e.g., `app.py` or `main.py`).
    ```bash
    cd server/
    uvicorn main:app --reload
    ```
    This will start the API server.

## Server API Endpoints

- **`/predict`** (POST):

  - **Description:** Processes an uploaded image and returns detected cars.
  - **Endpoint:** `POST /predict`
  - **Request:**
    - `Content-Type: multipart/form-data`
    - Body:
      - `file`: Image file (JPEG, PNG, etc.)
  - **Output Fields:**
    - `detections` (Array): List of detected cars.
    - `image_filename` (String): Name of the processed image file.
    - `object_count` (Integer): Number of cars detected in the image.
    - `annotated_image_base64` (String): Base64-encoded JPEG image with detection boxes drawn (may be null).
  - **Success Response (Status 200):**
    ```json
    {
      "detections": [
        {
          "class_name": "car",
          "confidence": 0.92,
          "bounding_box": {
            "x_min": 125.4,
            "y_min": 80.2,
            "x_max": 350.7,
            "y_max": 195.6
          }
        }
        // ... other detections (if any)
      ],
      "image_filename": "parking_lot_image.jpg",
      "object_count": 1,
      "annotated_image_base64": "base64_encoded_image_data..."
    }
    ```
  - **Error Responses:**
    - **Status 400:** Invalid request (non-image file, corrupted image).
      ```json
      {
        "detail": "Invalid file type. Please upload an image."
      }
      ```
    - **Status 503:** Service unavailable (model not loaded).
      ```json
      {
        "detail": "YOLO model is not loaded. Cannot process requests."
      }
      ```
    - **Status 500:** Internal server error.
      ```json
      {
        "detail": "Error during model prediction: [error details]"
      }
      ```
  - **Example `curl`:**
    ```bash
    curl -X POST -F "file=@/path/to/your_image.jpg" http://localhost:8000/predict
    ```

- **`/health`** (GET):
  - Description: Verifies the API is operational and the model is loaded correctly.
  - Endpoint: `GET /health`
  - **Success Response (Status 200):** API is functioning properly.
    ```json
    {
      "status": "ok",
      "message": "API is running and model is loaded."
    }
    ```
  - **Error Response (Status 503):** Model loading failed.
    ```json
    {
      "status": "error",
      "message": "YOLO model not loaded."
    }
    ```
